{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11112949,"sourceType":"datasetVersion","datasetId":6928687}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-04-13T10:33:26.410424Z\",\"iopub.execute_input\":\"2025-04-13T10:33:26.410725Z\",\"iopub.status.idle\":\"2025-04-13T10:33:30.539012Z\",\"shell.execute_reply.started\":\"2025-04-13T10:33:26.410702Z\",\"shell.execute_reply\":\"2025-04-13T10:33:30.537836Z\"}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2025-04-15T06:08:33.666183Z\",\"iopub.execute_input\":\"2025-04-15T06:08:33.666533Z\"}}\n# -*- coding: utf-8 -*-\n\"\"\"improvised code .ipynb\n\nAutomatically generated by Colab.research.google.com/drive/1-YjmRM1VCWsfbayXUIazTKDYn-bTRnBJ\n\"\"\"\n\n# %% [code]\n!pip install requests\n!pip install tabulate\n!pip install scikit-learn xgboost\n\n# %% [code]\n# Import libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport glob\nimport re\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost as xgb\nfrom sklearn.ensemble import VotingRegressor\n\n# =========================\n# üîπ 1Ô∏è‚É£ Load Data Function (improved)\n# =========================\ndef load_data(folder_path):\n    all_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n    data_list = []\n\n    for file in sorted(all_files, key=lambda x: float(re.search(r\"(\\d+\\.\\d+)\", x).group())):\n        df = pd.read_csv(file)\n        time_value = float(re.search(r\"(\\d+\\.\\d+)\", file).group())\n        df[\"time\"] = time_value\n        data_list.append(df)\n\n    full_data = pd.concat(data_list, ignore_index=True)\n    # Sort by time to ensure proper time series order\n    full_data = full_data.sort_values(by=[\"time\", \"nodenumber\"]).reset_index(drop=True)\n    return full_data\n\n# =========================\n# üîπ 2Ô∏è‚É£ Feature Engineering (improved)\n# =========================\ndef feature_engineering(df):\n    df.columns = df.columns.str.strip().str.lower()\n    target = \"total-temperature\"\n    if target not in df.columns:\n        raise KeyError(f\"‚ùå Target column '{target}' not found! Available columns: {df.columns.tolist()}\")\n\n    features = [\n        \"nodenumber\", \"x-coordinate\", \"y-coordinate\", \"total-pressure\",\n        \"total-energy\", \"heat-flux\", \"dx-velocity-dy\", \"dy-velocity-dy\",\n        \"dp-dx\", \"dp-dy\", \"time\"\n    ]\n\n    features = [f for f in features if f in df.columns]\n\n    # Enhanced time series features\n    df[\"temp_lag1\"] = df.groupby(\"nodenumber\")[target].shift(1)\n    df[\"temp_lag2\"] = df.groupby(\"nodenumber\")[target].shift(2)\n    df[\"temp_lag3\"] = df.groupby(\"nodenumber\")[target].shift(3)\n\n    # Exponential weighted moving averages (give more weight to recent observations)\n    df[\"temp_ewma3\"] = df.groupby(\"nodenumber\")[target].transform(lambda x: x.ewm(span=3).mean())\n    df[\"temp_ewma5\"] = df.groupby(\"nodenumber\")[target].transform(lambda x: x.ewm(span=5).mean())\n\n    # Standard moving averages\n    df[\"temp_ma3\"] = df.groupby(\"nodenumber\")[target].transform(lambda x: x.rolling(window=3).mean())\n    df[\"temp_ma5\"] = df.groupby(\"nodenumber\")[target].transform(lambda x: x.rolling(window=5).mean())\n\n    # Add rolling standard deviation to capture volatility\n    df[\"temp_std3\"] = df.groupby(\"nodenumber\")[target].transform(lambda x: x.rolling(window=3).std())\n\n    # Rate of change features\n    df[\"temp_roc1\"] = df.groupby(\"nodenumber\")[target].pct_change(1)\n    df[\"temp_roc3\"] = df.groupby(\"nodenumber\")[target].pct_change(3)\n\n    # Create time-based features if time represents actual timestamps\n    df[\"time_diff\"] = df.groupby(\"nodenumber\")[\"time\"].diff()\n\n    # Drop rows with NaN values (first few rows for each node)\n    df.dropna(inplace=True)\n\n    return df, features, target\n\n# =========================\n# üîπ 3Ô∏è‚É£ Time Series Validation Split (improved)\n# =========================\ndef time_series_split(df, test_ratio=0.2):\n    # Get unique time points\n    time_points = df[\"time\"].unique()\n    time_points.sort()\n\n    # Calculate split point\n    split_idx = int(len(time_points) * (1 - test_ratio))\n    train_times = time_points[:split_idx]\n    test_times = time_points[split_idx:]\n\n    # Split data\n    train_df = df[df[\"time\"].isin(train_times)]\n    test_df = df[df[\"time\"].isin(test_times)]\n\n    print(f\"üîç Training on time points: {train_times}\")\n    print(f\"üîç Testing on time points: {test_times}\")\n\n    return train_df, test_df\n\n# =========================\n# üîπ 4Ô∏è‚É£ Evaluate Model (improved)\n# =========================\ndef evaluate_model(name, y_true, y_pred):\n    mae = mean_absolute_error(y_true, y_pred)\n    mse = mean_squared_error(y_true, y_pred)\n    rmse = np.sqrt(mse)\n    r2 = r2_score(y_true, y_pred)\n    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n\n    print(f\"\\nüìä {name} Performance:\")\n    print(f\"‚úÖ MAE: {mae:.4f}\")\n    print(f\"‚úÖ RMSE: {rmse:.4f}\")\n    print(f\"‚úÖ MAPE: {mape:.2f}%\")\n    print(f\"‚úÖ R¬≤ Score: {r2:.4f}\")\n\n    return {\"mae\": mae, \"rmse\": rmse, \"r2\": r2, \"mape\": mape}\n\n# =========================\n# üîπ 5Ô∏è‚É£ Visualization Functions (improved)\n# =========================\ndef plot_error_vs_temperature(y_true, y_pred, model_name):\n    errors = np.abs(y_true - y_pred)\n    plt.figure(figsize=(10, 6))\n    sns.scatterplot(x=y_true, y=errors, alpha=0.6)\n\n    # Add trend line\n    z = np.polyfit(y_true, errors, 1)\n    p = np.poly1d(z)\n    plt.plot(sorted(y_true), p(sorted(y_true)), \"r--\", linewidth=2)\n\n    plt.xlabel(\"Actual Temperature\")\n    plt.ylabel(\"Absolute Error\")\n    plt.title(f\"Error vs Temperature ({model_name})\")\n    plt.grid(True)\n    plt.tight_layout()\n    plt.save()\n\ndef plot_predictions_vs_actual(y_true, y_pred, model_name):\n    plt.figure(figsize=(10, 6))\n\n    # Plot scatter with transparency\n    plt.scatter(y_true, y_pred, alpha=0.5)\n\n    # Add perfect prediction line\n    min_val = min(min(y_true), min(y_pred))\n    max_val = max(max(y_true), max(y_pred))\n    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n\n    plt.xlabel(\"Actual Temperature\")\n    plt.ylabel(\"Predicted Temperature\")\n    plt.title(f\"Predicted vs Actual Temperature ({model_name})\")\n    plt.grid(True)\n    plt.tight_layout()\n    plt.save()\n\ndef plot_time_series_predictions(test_df, y_pred, model_name, sample_nodes=200):\n    # Select a few random nodes to visualize\n    nodes = np.random.choice(test_df[\"nodenumber\"].unique(), min(sample_nodes, len(test_df[\"nodenumber\"].unique())), replace=False)\n\n    plt.figure(figsize=(12, 8))\n    for node in nodes:\n        node_data = test_df[test_df[\"nodenumber\"] == node].sort_values(\"time\")\n        node_idx = node_data.index\n\n        plt.plot(node_data[\"time\"], node_data[\"total-temperature\"], 'o-', label=f\"Node {node} - Actual\")\n        plt.plot(node_data[\"time\"], y_pred[node_idx], 'x--', label=f\"Node {node} - Predicted\")\n\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Temperature\")\n    plt.title(f\"Time Series Predictions for Sample Nodes ({model_name})\")\n    plt.legend(loc=\"best\")\n    plt.grid(True)\n    plt.tight_layout()\n    plt.save()\n\n# %% [code]\nif __name__ == \"__main__\":\n    folder_path = \"/kaggle/input/cfd-ml/data_for_ML_CSV\"  # Adjust this path as needed\n\n    try:\n        # Load data\n        print(\"üìÇ Loading data...\")\n        df = load_data(folder_path)\n        print(f\"üîç Loaded data shape: {df.shape}\")\n        print(f\"üîç Time range: {df['time'].min()} to {df['time'].max()}\")\n        print(f\"üîç Number of unique time points: {df['time'].nunique()}\")\n        print(f\"üîç Number of unique nodes: {df['nodenumber'].nunique()}\")\n\n        # Feature engineering\n        print(\"üîß Performing feature engineering...\")\n        df, features, target = feature_engineering(df)\n        print(f\"üîç Engineered data shape: {df.shape}\")\n        print(f\"üîç Features: {features}\")\n        print(f\"üîç Added time series features: {[col for col in df.columns if col not in features and col != target]}\")\n\n        # Split data using proper time series split\n        print(\"‚úÇÔ∏è Splitting data using time series approach...\")\n        train_df, test_df = time_series_split(df, test_ratio=0.2)\n        print(f\"üîç Train data shape: {train_df.shape}\")\n        print(f\"üîç Test data shape: {test_df.shape}\")\n\n        # Get all features (original + engineered)\n        all_features = [col for col in df.columns if col != target]\n\n        # Prepare training and testing sets\n        X_train, y_train = train_df[all_features], train_df[target]\n        X_test, y_test = test_df[all_features], test_df[target]\n\n        # Models\n        models = {\n            \"RandomForest\": RandomForestRegressor(n_estimators=100, random_state=42),\n        }\n            #\"GradientBoosting\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n            #\"XGBoost\": xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n        \n\n        # Ensemble model\n        estimators = [(name, model) for name, model in models.items()]\n        ensemble_model = VotingRegressor(estimators=estimators)\n\n        # Train ensemble\n        print(f\"\\nüöÄ Training Ensemble model...\")\n        ensemble_model.fit(X_train, y_train)\n\n        # Make predictions\n        print(\"üîÆ Making predictions...\")\n        y_pred = ensemble_model.predict(X_test)\n\n        # Evaluate the ensemble model\n        print(\"üìä Evaluating the ensemble model...\")\n        metrics = evaluate_model(\"Ensemble\", y_test.values, y_pred)\n\n        print(\"\\nüìà Creating visualizations...\")\n        plot_error_vs_temperature(y_test.values, y_pred, \"Ensemble\")\n        plot_predictions_vs_actual(y_test.values, y_pred, \"Ensemble\")\n        plot_time_series_predictions(test_df, y_pred, \"Ensemble\")\n\n        # Save metrics to file\n        metrics_df = pd.DataFrame([metrics])\n        metrics_df.to_csv(\"model_metrics.csv\")\n        print(\"‚úÖ Metrics saved to model_metrics.csv\")\n\n        print(\"\\n‚úÖ Pipeline completed successfully!\")\n\n    except Exception as e:\n        print(f\"‚ùå An error occurred: {str(e)}\")\n        import traceback\n        traceback.print_exc()","metadata":{"_uuid":"a155b4d0-dccc-4da5-af80-0f6905ae45f8","_cell_guid":"012c7299-bc63-41b1-b756-2d288e93dffe","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}